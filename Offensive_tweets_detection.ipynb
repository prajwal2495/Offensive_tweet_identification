{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data set....\n",
      "Done reading....\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data set....\")\n",
    "training_data_set = pd.read_csv(\"/Users/prajwalkrishn/Desktop/My_Computer/project - Dsci 601/Offensive_Tweet_Detection/Dataset/MOLID.csv\")\n",
    "print(\"Done reading....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  subtask_a subtask_b  \\\n",
       "0  This is Dubai's like Michael 's phone went pud...  Offensive       UNT   \n",
       "1  In fact, never was perceived to be thrown. Eve...  Offensive       TIN   \n",
       "2  Bhosadi I am your mother's husband. Look at yo...  Offensive       TIN   \n",
       "3  If you ask a dog? And the smoke is drawn throu...  Offensive       TIN   \n",
       "4  Where's Ram Kadam went to talk to the BJP and ...  Offensive       TIN   \n",
       "\n",
       "  subtask_c  \n",
       "0       NaN  \n",
       "1       IND  \n",
       "2       IND  \n",
       "3       IND  \n",
       "4       GRP  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = training_data_set[[\"tweet\"]]\n",
    "level_A_labels = training_data_set[[\"subtask_a\"]]\n",
    "level_B_labels = training_data_set.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "level_C_labels = training_data_set.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
    "\n",
    "All_Cleaned_tweets = copy.deepcopy(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  This is Dubai's like Michael 's phone went pud...\n",
       "1  In fact, never was perceived to be thrown. Eve...\n",
       "2  Bhosadi I am your mother's husband. Look at yo...\n",
       "3  If you ask a dog? And the smoke is drawn throu...\n",
       "4  Where's Ram Kadam went to talk to the BJP and ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subtask_a\n",
       "0  Offensive\n",
       "1  Offensive\n",
       "2  Offensive\n",
       "3  Offensive\n",
       "4  Offensive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_A_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_b\n",
       "0       UNT\n",
       "1       TIN\n",
       "2       TIN\n",
       "3       TIN\n",
       "4       TIN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_B_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c\n",
       "1       IND\n",
       "2       IND\n",
       "3       IND\n",
       "4       GRP\n",
       "5       IND"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_C_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai's like Michael 's phone went pud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact, never was perceived to be thrown. Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother's husband. Look at yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog? And the smoke is drawn throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where's Ram Kadam went to talk to the BJP and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  This is Dubai's like Michael 's phone went pud...\n",
       "1  In fact, never was perceived to be thrown. Eve...\n",
       "2  Bhosadi I am your mother's husband. Look at yo...\n",
       "3  If you ask a dog? And the smoke is drawn throu...\n",
       "4  Where's Ram Kadam went to talk to the BJP and ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Cleaned_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prajwalkrishn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "lancaster = LancasterStemmer()\n",
    "wordNet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_webTags_UserNames_Noise(tweet):\n",
    "    things_to_be_removed_from_tweets = ['URL','@USER','\\'ve','n\\'t','\\'s','\\'m']\n",
    "    \n",
    "    for things in things_to_be_removed_from_tweets:\n",
    "        tweet = tweet.replace(things,'')\n",
    "    \n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "def stop_words_removal(tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for token in tokens:\n",
    "        if token not in stop:\n",
    "            if token.replace(' ','') != '':\n",
    "                if len(token) > 1:\n",
    "                    cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def tokenize(tweet):\n",
    "    lower_cased_tweet = tweet.lower()\n",
    "    return word_tokenize(lower_cased_tweet)\n",
    "\n",
    "def stemming(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = lancaster.stem(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wordNet.lemmatize(token)\n",
    "        if len(token) > 1:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean...: 100%|██████████| 2499/2499 [00:00<00:00, 94190.92it/s]\n",
      "Tokenize..: 100%|██████████| 2499/2499 [00:00<00:00, 10717.77it/s]\n",
      "remove STOPWORDS...: 100%|██████████| 2499/2499 [00:00<00:00, 12121.59it/s]\n",
      "Stemming...: 100%|██████████| 2499/2499 [00:00<00:00, 12148.54it/s]\n",
      "Lemmatize...: 100%|██████████| 2499/2499 [00:00<00:00, 38572.61it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc = \"clean...\")\n",
    "All_Cleaned_tweets['tweet'] = tweets['tweet'].progress_apply(remove_webTags_UserNames_Noise)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tweet'].progress_apply(tokenize)\n",
    "\n",
    "tqdm.pandas(desc=\"remove STOPWORDS...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stop_words_removal)\n",
    "\n",
    "tqdm.pandas(desc=\"Stemming...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(stemming)\n",
    "\n",
    "tqdm.pandas(desc=\"Lemmatize...\")\n",
    "All_Cleaned_tweets['tokens'] = All_Cleaned_tweets['tokens'].progress_apply(lemmatization)\n",
    "\n",
    "text_vector = All_Cleaned_tweets['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is Dubai like Michael  phone went pudica</td>\n",
       "      <td>[duba, lik, michael, phon, went, pudic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact  never was perceived to be thrown  Eve...</td>\n",
       "      <td>[fact, nev, perceiv, thrown, everyth, mov, tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bhosadi I am your mother husband  Look at your...</td>\n",
       "      <td>[bhosad, moth, husband, look, moth, as, kil, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you ask a dog  And the smoke is drawn throu...</td>\n",
       "      <td>[ask, dog, smok, drawn, rub, goat, alon, kang,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where Ram Kadam went to talk to the BJP and no...</td>\n",
       "      <td>[ram, kadam, went, talk, bjp, behavy, mut, con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0     This is Dubai like Michael  phone went pudica    \n",
       "1  In fact  never was perceived to be thrown  Eve...   \n",
       "2  Bhosadi I am your mother husband  Look at your...   \n",
       "3  If you ask a dog  And the smoke is drawn throu...   \n",
       "4  Where Ram Kadam went to talk to the BJP and no...   \n",
       "\n",
       "                                              tokens  \n",
       "0            [duba, lik, michael, phon, went, pudic]  \n",
       "1  [fact, nev, perceiv, thrown, everyth, mov, tow...  \n",
       "2  [bhosad, moth, husband, look, moth, as, kil, d...  \n",
       "3  [ask, dog, smok, drawn, rub, goat, alon, kang,...  \n",
       "4  [ram, kadam, went, talk, bjp, behavy, mut, con...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Cleaned_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid(text_vector):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
    "    vectorizer = vectorizer.fit(untokenized_data)\n",
    "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
    "    return vectors\n",
    "  \n",
    "def get_vectors(vectors, labels, keyword):\n",
    "    if len(vectors) != len(labels):\n",
    "        print(\"Unmatching sizes!\")\n",
    "        return\n",
    "    result = list()\n",
    "    for vector, label in zip(vectors, labels):\n",
    "        if label == keyword:\n",
    "            result.append(vector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 2499/2499 [00:00<00:00, 556434.98it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_a = tfid(text_vector) # Numerical Vectors A\n",
    "labels_level_a = level_A_labels['subtask_a'].values.tolist() # Subtask A Labels\n",
    "\n",
    "vectors_level_b = get_vectors(vectors_level_a, labels_level_a, \"Offensive\") # Numerical Vectors B\n",
    "labels_level_b = level_B_labels['subtask_b'].values.tolist() # Subtask B Labels\n",
    "\n",
    "vectors_level_c = get_vectors(vectors_level_b, labels_level_b, \"TIN\") # Numerical Vectors C\n",
    "labels_level_c = level_C_labels['subtask_c'].values.tolist() # Subtask C Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(vectors_level_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     nan,\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'GRP',\n",
      "     'OTH',\n",
      "     'GRP',\n",
      "     'GRP',\n",
      "     'IND',\n",
      "     'IND',\n",
      "     'OTH',\n",
      "     'IND']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(labels_level_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit begins...\n",
      "fit complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.9636363636363636\n",
      "Test Accuracy: 0.6530612244897959\n",
      "Confusion Matrix:\n",
      "[[12 19  1]\n",
      " [16 83  4]\n",
      " [ 2  9  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.40      0.38      0.39        32\n",
      "         IND       0.75      0.81      0.78       103\n",
      "         OTH       0.17      0.08      0.11        12\n",
      "\n",
      "    accuracy                           0.65       147\n",
      "   macro avg       0.44      0.42      0.42       147\n",
      "weighted avg       0.62      0.65      0.64       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5)\n",
    "params = {'criterion':['gini','entropy']}\n",
    "classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "classifier = classifier.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifier.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "classNames = np.unique(test_labels)\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifiersvc = SVC()\n",
    "print(classifiersvc.get_params().keys())\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "classifierGrid = GridSearchCV(classifiersvc, param_grid, refit = True, verbose=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "matrix = confusion_matrix(test_labels, test_predictions)\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))\n",
    "\n",
    "plottedCM = plot_confusion_matrix(classifierGrid, test_vectors, test_labels,display_labels=classNames, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RandomForest model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierRFC = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "print(classifierRFC.get_params().keys())\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 700],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierRFC, param_grid, refit = True, verbose=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MNB model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierMNB = MultinomialNB()\n",
    "# print(classifierMNB.get_params().keys())\n",
    "param_grid = { \n",
    "    'alpha': [1, 10, 100, 1000],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierMNB, param_grid, refit = True, verbose=2, n_jobs=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN model experiment begins ...\")\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_a[:], labels_level_a[:], train_size=0.70)\n",
    "\n",
    "#train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_b[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "print(\"fit begins...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "classifierKNN = KNeighborsClassifier()\n",
    "#print(classifierKNN.get_params().keys())\n",
    "param_grid = { \n",
    "    'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "classifierGrid = GridSearchCV(classifierKNN, param_grid, refit = True, verbose=2, n_jobs=2)\n",
    "classifierGrid.fit(train_vectors, train_labels)\n",
    "classifierGrid = classifierGrid.best_estimator_\n",
    "print(\"fit complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "accuracy = accuracy_score(train_labels, classifierGrid.predict(train_vectors))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "test_predictions = classifierGrid.predict(test_vectors)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\", )\n",
    "print(confusion_matrix(test_labels, test_predictions))\n",
    "print(classification_report(test_labels,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
